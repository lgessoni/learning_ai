{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_rel_path = '../datasets/notmnist/notMNIST.pickle'\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "run = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle location: C:\\Users\\lgess\\Documents\\repo\\learning_ai\\datasets\\notmnist\\notMNIST.pickle\n",
      "dict_keys(['valid_labels', 'test_labels', 'train_dataset', 'valid_dataset', 'train_labels', 'test_dataset'])\n"
     ]
    }
   ],
   "source": [
    "# Discover pickle path\n",
    "file_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "pickle_path = os.path.abspath(os.path.join(file_dir, pickle_rel_path))\n",
    "print('Pickle location:',pickle_path)\n",
    "\n",
    "# Load pickle\n",
    "with open(pickle_path,'rb') as file:\n",
    "    datasets = pickle.load(file)\n",
    "print(datasets.keys())\n",
    "\n",
    "# Set variables\n",
    "train_dataset = datasets['train_dataset']\n",
    "orig_train_labels = datasets['train_labels'] \n",
    "validate_dataset = datasets['valid_dataset']\n",
    "orig_validate_labels = datasets['valid_labels'] \n",
    "test_dataset = datasets['test_dataset']\n",
    "orig_test_labels = datasets['test_labels'] \n",
    "\n",
    "# Convert representation\n",
    "sample_width = train_dataset.shape[2]\n",
    "sample_height = train_dataset.shape[1]\n",
    "features_shape = [sample_height,sample_width,1]\n",
    "num_labels = 10\n",
    "def setup_xy(samples,labels):\n",
    "    x = samples.reshape(np.append([-1],features_shape));\n",
    "    y = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return (x,y)\n",
    "train_dataset,train_labels = setup_xy(datasets['train_dataset'],datasets['train_labels'])\n",
    "validate_dataset,validate_labels = setup_xy(datasets['valid_dataset'],datasets['valid_labels'])\n",
    "test_dataset,test_labels = setup_xy(datasets['test_dataset'],datasets['test_labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer: \n",
    "    \n",
    "    # Todo: add with tf.name_scope(self.name):\n",
    "    \n",
    "    def __init__(self, name, features_shape):\n",
    "        self.name, self.features_shape = name, features_shape\n",
    "        self.w, self.b = None, None\n",
    "        \n",
    "    def create_vars(self):\n",
    "        with tf.name_scope(self.name):\n",
    "            w_shape, b_shape = self.wb_shapes()\n",
    "            initializer = tf.glorot_normal_initializer()\n",
    "            if not w_shape is None:\n",
    "                self.w = tf.get_variable(name=\"W\"+self.name, shape=w_shape, \n",
    "                                         dtype=tf.float32, initializer=initializer)\n",
    "            if not b_shape is None:    \n",
    "                self.b = tf.get_variable(name=\"B\"+self.name, shape=b_shape, \n",
    "                                         dtype=tf.float32, initializer=initializer)\n",
    "    \n",
    "    def inference(self, x):\n",
    "        with tf.name_scope(self.name):\n",
    "            return self._inference(x)\n",
    "            \n",
    "    def training(self, x, y, keep_prob):\n",
    "        with tf.name_scope(self.name):\n",
    "            return self._training(x, y, keep_prob)\n",
    "    \n",
    "    def wb_shapes(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def out_features_shape(self):\n",
    "        raise NotImplementedError() \n",
    "            \n",
    "    def _inference(self, x):\n",
    "        raise NotImplementedError() \n",
    "            \n",
    "    def _training(self, x, y, keep_prob):\n",
    "        raise NotImplementedError() \n",
    "        \n",
    "class InputLayer(Layer):           \n",
    "        \n",
    "    def __init__(self, layerNum, features_shape):\n",
    "        name = str(layerNum)+\"In\"\n",
    "        super().__init__(name, features_shape)    \n",
    "        \n",
    "    def wb_shapes(self):\n",
    "        return (None, None)\n",
    "        \n",
    "    def out_features_shape(self):\n",
    "        return features_shape\n",
    "        \n",
    "    def _inference(self, x):\n",
    "        return x\n",
    "            \n",
    "    def _training(self, x, y, keep_prob):\n",
    "        return x\n",
    "        \n",
    "class FlattenLayer(Layer): # Flattens x into [num_samples,-1]            \n",
    "        \n",
    "    def __init__(self, layerNum, features_shape):\n",
    "        name = str(layerNum)+\"Flat\"\n",
    "        super().__init__(name, features_shape)    \n",
    "        \n",
    "    def wb_shapes(self):\n",
    "        return (None, None)\n",
    "        \n",
    "    def out_features_shape(self):\n",
    "        return [np.prod(self.features_shape)]\n",
    "        \n",
    "    def _inference(self, x):\n",
    "        out_features_num = self.out_features_shape()[0]\n",
    "        return tf.reshape(x, [-1, out_features_num])\n",
    "            \n",
    "    def _training(self, x, y, keep_prob):\n",
    "        return self._inference(x)\n",
    "\n",
    "class TradHiddenLayer(Layer): \n",
    "    \n",
    "    # TODO accept generic shape by using tensordot? (less performance?)\n",
    "    \n",
    "    def __init__(self, layerNum, features_shape, neurons):\n",
    "        name = str(layerNum)+\"Relu\"\n",
    "        super().__init__(name, features_shape)\n",
    "        self.neurons = neurons\n",
    "        \n",
    "    def wb_shapes(self):\n",
    "        return ( np.append(self.features_shape,[self.neurons]), [self.neurons] )\n",
    "        \n",
    "    def out_features_shape(self):\n",
    "        return [self.neurons]\n",
    "            \n",
    "    def _inference(self, x): \n",
    "        z = tf.matmul(x, self.w)+self.b \n",
    "        return tf.nn.relu(z)\n",
    "            \n",
    "    def _training(self, x, y, keep_prob):\n",
    "        if keep_prob<1:\n",
    "            x = tf.nn.dropout(x, keep_prob)\n",
    "        return self._inference(x)\n",
    "        \n",
    "class ConvHiddenLayer(Layer):      \n",
    "    \n",
    "    # TODO different dropout for conv?\n",
    "        \n",
    "    def __init__(self, layerNum, features_shape, kernel_size, depth, stride_size, padding):\n",
    "        name = str(layerNum)+\"Conv\"\n",
    "        super().__init__(name, features_shape)\n",
    "        self.kernel_size = kernel_size; self.depth = depth\n",
    "        self.stride_size = stride_size; self.padding = padding\n",
    "            \n",
    "    def wb_shapes(self):\n",
    "        return ( [self.kernel_size,self.kernel_size,self.features_shape[-1],self.depth], [self.depth] )\n",
    "        \n",
    "    def out_features_shape(self):\n",
    "        if self.padding == \"SAME\":\n",
    "            return [math.floor(self.features_shape[0]/self.stride_size),\n",
    "                    math.floor(self.features_shape[1]/self.stride_size),\n",
    "                    self.depth]\n",
    "        else:\n",
    "            return [math.floor((self.features_shape[0]-self.kernel_size)/self.stride_size) +1,\n",
    "                    math.floor((self.features_shape[1]-self.kernel_size)/self.stride_size) +1,\n",
    "                    self.depth]\n",
    "            \n",
    "    def _inference(self, x):\n",
    "        conv = tf.nn.conv2d(input=x, filter=self.w, padding=self.padding,\n",
    "                            strides=[1,self.stride_size,self.stride_size,1] )\n",
    "        return tf.nn.relu(conv+self.b)\n",
    "            \n",
    "    def _training(self, x, y, keep_prob): \n",
    "        if keep_prob<1:\n",
    "            x = tf.nn.dropout(x, keep_prob)\n",
    "        return self._inference(x)\n",
    "                          \n",
    "class OutputLayer(TradHiddenLayer):\n",
    "    \n",
    "    # TODO accept generic shape by using tensordot? (less performance?)\n",
    "    # TODO Should have dropout?\n",
    "    \n",
    "    def __init__(self, layerNum, features_shape, num_labels):\n",
    "        name = str(layerNum)+\"SMax\"\n",
    "        super().__init__(name, features_shape, num_labels)\n",
    "            \n",
    "    def _inference(self, x):\n",
    "        z = tf.matmul(x, self.w)+self.b \n",
    "        return tf.nn.softmax(z)\n",
    "                             \n",
    "    def _training(self, x, y, keep_prob):          \n",
    "        z = tf.matmul(x, self.w)+self.b \n",
    "        return tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits=z, labels=y))\n",
    "            \n",
    "        \n",
    "class Model:\n",
    "                          \n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers   \n",
    "        \n",
    "    def create_vars(self):\n",
    "        for layer in self.layers:\n",
    "            layer.create_vars()\n",
    "                          \n",
    "    def inference(self, x):\n",
    "        with tf.name_scope(\"Inference\"):\n",
    "            for layer in self.layers:\n",
    "                x = layer.inference(x)\n",
    "            return x\n",
    "                          \n",
    "    def loss(self, x, y, keep_prob, regularization):\n",
    "        l2 = 0\n",
    "        with tf.name_scope(\"Loss\"):\n",
    "            for layer in self.layers:\n",
    "                x = layer.training(x, y, keep_prob)\n",
    "                if (not layer.w is None) and (regularization > 0):\n",
    "                    l2 = l2 + tf.nn.l2_loss(layer.w)/tf.cast(tf.size(layer.w),tf.float32)\n",
    "            return x + regularization*l2\n",
    "        \n",
    "    def training_step(self, x, y, learning_rate, keep_prob, regularization):\n",
    "        with tf.name_scope(\"TrainingStep\"):\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "            loss = self.loss(x, y, keep_prob, regularization)\n",
    "            tf.summary.scalar('loss', loss)\n",
    "            return optimizer.minimize(loss)\n",
    "    \n",
    "class SessionRunner:\n",
    "    \n",
    "    # TODO stop when validation loss stops to grow\n",
    "    \n",
    "    def __init__(self, model, run):\n",
    "        \n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            \n",
    "            self.model = model\n",
    "            self.session = tf.Session()\n",
    "            self.log_writer = tf.summary.FileWriter(\"/tf_log/\"+str(run), self.graph)\n",
    "            \n",
    "            self.x = tf.placeholder(dtype=tf.float32,shape=np.append(None,features_shape), name=\"x\")\n",
    "            self.y = tf.placeholder(dtype=tf.float32,shape=np.append(None,[num_labels]), name=\"y\")\n",
    "                        \n",
    "            model.create_vars() \n",
    "            self.inference = model.inference(self.x)\n",
    "\n",
    "    def train(self, train_dataset, train_labels, batch_size=200, iterations=1000,\n",
    "              learning_rate=0.5, keep_prob=1, regularization=0):\n",
    "        \n",
    "        index = 0\n",
    "        num_data = len(train_dataset)\n",
    "        if batch_size > num_data:\n",
    "            batch_size = num_data\n",
    "        \n",
    "        with self.graph.as_default():            \n",
    "            \n",
    "            init = tf.global_variables_initializer()\n",
    "            step_op = model.training_step(self.x, self.y, learning_rate, keep_prob, regularization)\n",
    "            summary_op = tf.summary.merge_all()\n",
    "        \n",
    "        self.session.run(init)\n",
    "        \n",
    "        for i in range(iterations):   \n",
    "            if 2*batch_size < num_data:\n",
    "                index = (i*batch_size)%(len(train_dataset)-batch_size)\n",
    "            feed_dict={self.x: train_dataset[index:index+batch_size], \n",
    "                       self.y: train_labels[index:index+batch_size]}      \n",
    "            _,summary = self.session.run([step_op, summary_op], feed_dict)\n",
    "            self.log_writer.add_summary(summary, i)\n",
    "        print(\"done\")\n",
    "                \n",
    "    def predict(self, dataset):\n",
    "        return self.session.run(self.inference, feed_dict={self.x: dataset})\n",
    "        \n",
    "    def evaluate(self, dataset, labels):\n",
    "        print(self.__accuracy(predicted=self.predict(dataset), expected=labels))\n",
    "        \n",
    "    def __accuracy(self, predicted, expected):\n",
    "        return np.mean(np.argmax(predicted,1) == np.argmax(expected,1))\n",
    "         \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(features_shape, num_labels):\n",
    "                            \n",
    "    layers = []; layer_num = 0;\n",
    "    num_features = np.prod(features_shape)\n",
    "    \n",
    "    # InputLayer\n",
    "    layer = InputLayer(layer_num, features_shape)\n",
    "    layers.append(layer); layer_num+=1\n",
    "    # ConvHiddenLayer\n",
    "    layer = ConvHiddenLayer(layer_num, layer.out_features_shape(), \n",
    "                            kernel_size=7, depth=16, stride_size=1, padding=\"VALID\")\n",
    "    layers.append(layer); layer_num+=1\n",
    "    # ConvHiddenLayer\n",
    "    layer = ConvHiddenLayer(layer_num, layer.out_features_shape(), \n",
    "                            kernel_size=7, depth=32, stride_size=1, padding=\"VALID\")\n",
    "    layers.append(layer); layer_num+=1\n",
    "    # FlattenLayer\n",
    "    layer = FlattenLayer(layer_num, layer.out_features_shape())\n",
    "    layers.append(layer); layer_num+=1\n",
    "    # TradHiddenLayer\n",
    "    layer = TradHiddenLayer(layer_num, layer.out_features_shape(), neurons=64)\n",
    "    layers.append(layer); layer_num+=1\n",
    "    # TradHiddenLayer\n",
    "    layer = TradHiddenLayer(layer_num, layer.out_features_shape(), neurons=64)\n",
    "    layers.append(layer); layer_num+=1\n",
    "    # OutputLayer\n",
    "    layer = OutputLayer(layer_num, layer.out_features_shape(), num_labels)\n",
    "    layers.append(layer); \n",
    "\n",
    "    return Model(layers=layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    session_runner.session.close()\n",
    "except:\n",
    "    None\n",
    "    \n",
    "model = create_classifier(features_shape, num_labels)\n",
    "session_runner = SessionRunner(model, run); num_t_samples = 200000\n",
    "\n",
    "run += 1\n",
    "session_runner.train(train_dataset[0:num_t_samples], train_labels[0:num_t_samples], \n",
    "                     batch_size=30, iterations=5000, \n",
    "                     learning_rate=0.05, keep_prob=1, regularization=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8716\n",
      "0.9379\n"
     ]
    }
   ],
   "source": [
    "num_t_samples = 25000\n",
    "#session_runner.evaluate(train_dataset[0:num_t_samples], train_labels[0:num_t_samples])\n",
    "session_runner.evaluate(validate_dataset, validate_labels)\n",
    "session_runner.evaluate(test_dataset, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x291c8914588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_sample(sample_image, label):\n",
    "    plt.figure()\n",
    "    plt.imshow(sample_image)\n",
    "    print('Label:', np.argmax(label))\n",
    "\n",
    "index=0\n",
    "show_sample(validate_dataset[index].reshape(features_shape[0:2]),validate_labels[index])\n",
    "np.argmax(session_runner.predict([validate_dataset[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
