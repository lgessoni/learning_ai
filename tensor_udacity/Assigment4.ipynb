{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_rel_path = '../datasets/notmnist/notMNIST.pickle'\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "run = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle location: C:\\Users\\lgess\\Documents\\repo\\learning_ai\\datasets\\notmnist\\notMNIST.pickle\n",
      "dict_keys(['test_labels', 'train_dataset', 'valid_dataset', 'train_labels', 'valid_labels', 'test_dataset'])\n"
     ]
    }
   ],
   "source": [
    "# Discover pickle path\n",
    "file_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "pickle_path = os.path.abspath(os.path.join(file_dir, pickle_rel_path))\n",
    "print('Pickle location:',pickle_path)\n",
    "\n",
    "# Load pickle\n",
    "with open(pickle_path,'rb') as file:\n",
    "    datasets = pickle.load(file)\n",
    "print(datasets.keys())\n",
    "\n",
    "# Set variables\n",
    "train_dataset = datasets['train_dataset']\n",
    "orig_train_labels = datasets['train_labels'] \n",
    "validate_dataset = datasets['valid_dataset']\n",
    "orig_validate_labels = datasets['valid_labels'] \n",
    "test_dataset = datasets['test_dataset']\n",
    "orig_test_labels = datasets['test_labels'] \n",
    "\n",
    "# Convert representation\n",
    "sample_width = train_dataset.shape[2]\n",
    "sample_height = train_dataset.shape[1]\n",
    "features_shape = [sample_height,sample_width,1]\n",
    "num_labels = 10\n",
    "def setup_xy(samples,labels):\n",
    "    x = samples.reshape(np.append([-1],features_shape));\n",
    "    y = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return (x,y)\n",
    "train_dataset,train_labels = setup_xy(datasets['train_dataset'],datasets['train_labels'])\n",
    "validate_dataset,validate_labels = setup_xy(datasets['valid_dataset'],datasets['valid_labels'])\n",
    "test_dataset,test_labels = setup_xy(datasets['test_dataset'],datasets['test_labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer: \n",
    "    \n",
    "    # Todo: add with tf.name_scope(self.name):\n",
    "    \n",
    "    def __init__(self, name, features_shape):\n",
    "        self.name, self.features_shape = name, features_shape\n",
    "        self.w, self.b = None, None\n",
    "        \n",
    "    def create_vars(self):\n",
    "        with tf.name_scope(self.name):\n",
    "            w_shape, b_shape = self.wb_shapes()\n",
    "            initializer = tf.glorot_normal_initializer()\n",
    "            if not w_shape is None:\n",
    "                self.w = tf.get_variable(name=\"W\"+self.name, shape=w_shape, \n",
    "                                         dtype=tf.float32, initializer=initializer)\n",
    "            if not b_shape is None:    \n",
    "                self.b = tf.get_variable(name=\"B\"+self.name, shape=b_shape, \n",
    "                                         dtype=tf.float32, initializer=initializer)\n",
    "    \n",
    "    def inference(self, x):\n",
    "        with tf.name_scope(self.name):\n",
    "            return self._inference(x)\n",
    "            \n",
    "    def training(self, x, y, keep_prob):\n",
    "        with tf.name_scope(self.name):\n",
    "            return self._training(x, y, keep_prob)\n",
    "    \n",
    "    def wb_shapes(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def out_features_shape(self):\n",
    "        raise NotImplementedError() \n",
    "            \n",
    "    def _inference(self, x):\n",
    "        raise NotImplementedError() \n",
    "            \n",
    "    def _training(self, x, y, keep_prob):\n",
    "        raise NotImplementedError() \n",
    "        \n",
    "class InputLayer(Layer):           \n",
    "        \n",
    "    def __init__(self, layerNum, features_shape):\n",
    "        name = str(layerNum)+\"In\"\n",
    "        super().__init__(name, features_shape)    \n",
    "        \n",
    "    def wb_shapes(self):\n",
    "        return (None, None)\n",
    "        \n",
    "    def out_features_shape(self):\n",
    "        return features_shape\n",
    "        \n",
    "    def _inference(self, x):\n",
    "        return x\n",
    "            \n",
    "    def _training(self, x, y, keep_prob):\n",
    "        return x\n",
    "        \n",
    "class FlattenLayer(Layer): # Flattens x into [num_samples,-1]            \n",
    "        \n",
    "    def __init__(self, layerNum, features_shape):\n",
    "        name = str(layerNum)+\"Flat\"\n",
    "        super().__init__(name, features_shape)    \n",
    "        \n",
    "    def wb_shapes(self):\n",
    "        return (None, None)\n",
    "        \n",
    "    def out_features_shape(self):\n",
    "        return [np.prod(self.features_shape)]\n",
    "        \n",
    "    def _inference(self, x):\n",
    "        out_features_num = self.out_features_shape()[0]\n",
    "        return tf.reshape(x, [-1, out_features_num])\n",
    "            \n",
    "    def _training(self, x, y, keep_prob):\n",
    "        return self._inference(x)\n",
    "\n",
    "class TradHiddenLayer(Layer): \n",
    "    \n",
    "    # TODO accept generic shape by using tensordot? (less performance?)\n",
    "    \n",
    "    def __init__(self, layerNum, features_shape, neurons):\n",
    "        name = str(layerNum)+\"Relu\"\n",
    "        super().__init__(name, features_shape)\n",
    "        self.neurons = neurons\n",
    "        \n",
    "    def wb_shapes(self):\n",
    "        return ( np.append(self.features_shape,[self.neurons]), [self.neurons] )\n",
    "        \n",
    "    def out_features_shape(self):\n",
    "        return [self.neurons]\n",
    "            \n",
    "    def _inference(self, x): \n",
    "        z = tf.matmul(x, self.w)+self.b \n",
    "        return tf.nn.relu(z)\n",
    "            \n",
    "    def _training(self, x, y, keep_prob):\n",
    "        if keep_prob<1:\n",
    "            x = tf.nn.dropout(x, keep_prob)\n",
    "        return self._inference(x)\n",
    "        \n",
    "class ConvHiddenLayer(Layer):      \n",
    "    \n",
    "    # TODO different dropout for conv?\n",
    "        \n",
    "    def __init__(self, layerNum, features_shape, kernel_size, stride_size, depth, padding):\n",
    "        name = str(layerNum)+\"Conv\"\n",
    "        super().__init__(name, features_shape)\n",
    "        self.kernel_size = kernel_size; self.depth = depth\n",
    "        self.stride_size = stride_size; self.padding = padding\n",
    "            \n",
    "    def wb_shapes(self):\n",
    "        return ( [self.kernel_size,self.kernel_size,self.features_shape[-1],self.depth], [self.depth] )\n",
    "        \n",
    "    def out_features_shape(self):\n",
    "        if self.padding == \"SAME\":\n",
    "            return [math.floor(self.features_shape[0]/self.stride_size),\n",
    "                    math.floor(self.features_shape[1]/self.stride_size),\n",
    "                    self.depth]\n",
    "        else:\n",
    "            return [math.floor((self.features_shape[0]-self.kernel_size)/self.stride_size) +1,\n",
    "                    math.floor((self.features_shape[1]-self.kernel_size)/self.stride_size) +1,\n",
    "                    self.depth]\n",
    "            \n",
    "    def _inference(self, x):\n",
    "        conv = tf.nn.conv2d(input=x, filter=self.w, padding=self.padding,\n",
    "                            strides=[1,self.stride_size,self.stride_size,1] )\n",
    "        return tf.nn.relu(conv+self.b)\n",
    "            \n",
    "    def _training(self, x, y, keep_prob): \n",
    "        if keep_prob<1:\n",
    "            x = tf.nn.dropout(x, keep_prob)\n",
    "        return self._inference(x)\n",
    "    \n",
    "class PoolHiddenLayer(Layer):      \n",
    "    \n",
    "    # TODO add dropout?\n",
    "        \n",
    "    def __init__(self, layerNum, features_shape, kernel_size, stride_size, padding):\n",
    "        name = str(layerNum)+\"Pool\"\n",
    "        super().__init__(name, features_shape)\n",
    "        self.kernel_size = kernel_size; \n",
    "        self.stride_size = stride_size; \n",
    "        self.padding = padding\n",
    "            \n",
    "    def wb_shapes(self):\n",
    "        return (None,None)\n",
    "        \n",
    "    def out_features_shape(self):\n",
    "        if self.padding == \"SAME\":\n",
    "            return [math.floor(self.features_shape[0]/self.stride_size),\n",
    "                    math.floor(self.features_shape[1]/self.stride_size),\n",
    "                    self.features_shape[-1]]\n",
    "        else:\n",
    "            return [math.floor((self.features_shape[0]-self.kernel_size)/self.stride_size) +1,\n",
    "                    math.floor((self.features_shape[1]-self.kernel_size)/self.stride_size) +1,\n",
    "                    self.features_shape[-1]]\n",
    "            \n",
    "    def _inference(self, x):\n",
    "        return tf.nn.max_pool(value=x, ksize=[1,self.kernel_size,self.kernel_size,1], \n",
    "                              strides=[1,self.stride_size,self.stride_size,1], padding=self.padding)\n",
    "                    \n",
    "    def _training(self, x, y, keep_prob): \n",
    "        #if keep_prob<1:\n",
    "        #    x = tf.nn.dropout(x, keep_prob)\n",
    "        return self._inference(x)\n",
    "\n",
    "                          \n",
    "class OutputLayer(TradHiddenLayer):\n",
    "    \n",
    "    # TODO accept generic shape by using tensordot? (less performance?)\n",
    "    # TODO Should have dropout?\n",
    "    \n",
    "    def __init__(self, layerNum, features_shape, num_labels):\n",
    "        name = str(layerNum)+\"SMax\"\n",
    "        super().__init__(name, features_shape, num_labels)\n",
    "            \n",
    "    def _inference(self, x):\n",
    "        z = tf.matmul(x, self.w)+self.b \n",
    "        return tf.nn.softmax(z)\n",
    "                             \n",
    "    def _training(self, x, y, keep_prob):          \n",
    "        z = tf.matmul(x, self.w)+self.b \n",
    "        return tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits=z, labels=y))\n",
    "            \n",
    "        \n",
    "class Model:\n",
    "                          \n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers   \n",
    "        \n",
    "    def create_vars(self):\n",
    "        for layer in self.layers:\n",
    "            layer.create_vars()\n",
    "                          \n",
    "    def inference(self, x):\n",
    "        with tf.name_scope(\"Inference\"):\n",
    "            for layer in self.layers:\n",
    "                x = layer.inference(x)\n",
    "            return x\n",
    "                          \n",
    "    def loss(self, x, y, keep_prob, regularization):\n",
    "        l2 = 0\n",
    "        with tf.name_scope(\"Loss\"):\n",
    "            for layer in self.layers:\n",
    "                x = layer.training(x, y, keep_prob)\n",
    "                if (not layer.w is None) and (regularization > 0):\n",
    "                    l2 = l2 + tf.nn.l2_loss(layer.w)/tf.cast(tf.size(layer.w),tf.float32)\n",
    "            return x + regularization*l2\n",
    "        \n",
    "    def training_step(self, x, y, learning_rate, keep_prob, regularization):\n",
    "        with tf.name_scope(\"TrainingStep\"):\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "            loss = self.loss(x, y, keep_prob, regularization)\n",
    "            tf.summary.scalar('loss', loss)\n",
    "            return optimizer.minimize(loss)\n",
    "    \n",
    "class SessionRunner:\n",
    "    \n",
    "    # TODO stop when validation loss stops to grow\n",
    "    \n",
    "    def __init__(self, model, run):\n",
    "        \n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            \n",
    "            self.model = model\n",
    "            self.session = tf.Session()\n",
    "            self.log_writer = tf.summary.FileWriter(\"/tf_log/\"+str(run), self.graph)\n",
    "            \n",
    "            self.x = tf.placeholder(dtype=tf.float32,shape=np.append(None,features_shape), name=\"x\")\n",
    "            self.y = tf.placeholder(dtype=tf.float32,shape=np.append(None,[num_labels]), name=\"y\")\n",
    "                        \n",
    "            model.create_vars() \n",
    "            self.inference_op = model.inference(self.x) \n",
    "            equal_op = tf.equal(tf.argmax(self.inference_op, 1), tf.argmax(self.y, 1))\n",
    "            self.eval_op = tf.reduce_mean(tf.cast(equal_op, tf.float32))\n",
    "            tf.summary.scalar('acc', self.eval_op)            \n",
    "\n",
    "    def train(self, train_dataset, train_labels, \n",
    "              validate_dataset, validate_labels,\n",
    "              batch_size=200, iterations=1000,\n",
    "              learning_rate=0.5, keep_prob=1, regularization=0):\n",
    "        \n",
    "        index = 0\n",
    "        num_data = len(train_dataset)\n",
    "        if batch_size > num_data:\n",
    "            batch_size = num_data         \n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            init_op = tf.global_variables_initializer()\n",
    "            step_op = model.training_step(self.x, self.y, learning_rate, \n",
    "                                          keep_prob, regularization)\n",
    "            summary_op = tf.summary.merge_all()\n",
    "            \n",
    "        self.session.run(init_op)\n",
    "\n",
    "        for i in range(iterations):   \n",
    "            if 2*batch_size < num_data:\n",
    "                index = (i*batch_size)%(len(train_dataset)-batch_size)\n",
    "            feed_dict={self.x: train_dataset[index:index+batch_size], \n",
    "                       self.y: train_labels[index:index+batch_size]}      \n",
    "            self.session.run(step_op, feed_dict)\n",
    "            #_,summary = self.session.run([step_op, summary_op], feed_dict)\n",
    "            #self.log_writer.add_summary(summary, i)\n",
    "            if i%250==0:\n",
    "                feed_dict={self.x: validate_dataset, self.y: validate_labels}\n",
    "                _,summary = self.session.run([self.eval_op, summary_op], feed_dict)\n",
    "                self.log_writer.add_summary(summary, i)\n",
    "\n",
    "        print(\"done\")     \n",
    "        \n",
    "    def evaluate(self, x, y):\n",
    "        print(self.session.run(self.eval_op, {self.x: x, self.y: y}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(features_shape, num_labels):\n",
    "                            \n",
    "    layers = []; layer_num = 0;\n",
    "    num_features = np.prod(features_shape)\n",
    "    \n",
    "    # InputLayer\n",
    "    layer = InputLayer(layer_num, features_shape)\n",
    "    layers.append(layer); layer_num+=1\n",
    "    # ConvHiddenLayer\n",
    "    layer = ConvHiddenLayer(layer_num, layer.out_features_shape(), \n",
    "                            kernel_size=5, depth=16, stride_size=1, padding=\"SAME\")\n",
    "    layers.append(layer); layer_num+=1\n",
    "    # PoolHiddenLayer\n",
    "    layer = PoolHiddenLayer(layer_num, layer.out_features_shape(), \n",
    "                            kernel_size=2, stride_size=2, padding=\"VALID\")\n",
    "    layers.append(layer); layer_num+=1\n",
    "    # ConvHiddenLayer\n",
    "    layer = ConvHiddenLayer(layer_num, layer.out_features_shape(), \n",
    "                            kernel_size=5, depth=32, stride_size=1, padding=\"SAME\")\n",
    "    layers.append(layer); layer_num+=1\n",
    "    # PoolHiddenLayer\n",
    "    layer = PoolHiddenLayer(layer_num, layer.out_features_shape(), \n",
    "                            kernel_size=2, stride_size=2, padding=\"VALID\")\n",
    "    layers.append(layer); layer_num+=1\n",
    "    # FlattenLayer\n",
    "    layer = FlattenLayer(layer_num, layer.out_features_shape())\n",
    "    layers.append(layer); layer_num+=1\n",
    "    # TradHiddenLayer\n",
    "    layer = TradHiddenLayer(layer_num, layer.out_features_shape(), neurons=100)\n",
    "    layers.append(layer); layer_num+=1\n",
    "    # TradHiddenLayer\n",
    "    layer = TradHiddenLayer(layer_num, layer.out_features_shape(), neurons=100)\n",
    "    layers.append(layer); layer_num+=1    \n",
    "    # OutputLayer\n",
    "    layer = OutputLayer(layer_num, layer.out_features_shape(), num_labels)\n",
    "    layers.append(layer); \n",
    "\n",
    "    return Model(layers=layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session_runner.session.close()\n",
    "except:\n",
    "    None\n",
    "    \n",
    "model = create_classifier(features_shape, num_labels)\n",
    "session_runner = SessionRunner(model, run); \n",
    "num_t_samples = 200000; num_v_samples = 5000\n",
    "\n",
    "run += 1\n",
    "session_runner.train(train_dataset[0:num_t_samples], train_labels[0:num_t_samples],\n",
    "                     validate_dataset[0:num_v_samples], validate_labels[0:num_v_samples],\n",
    "                     batch_size=30, iterations=10000, \n",
    "                     learning_rate=0.05, keep_prob=1, regularization=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_t_samples = 20000\n",
    "#session_runner.evaluate(train_dataset[0:num_t_samples], train_labels[0:num_t_samples])\n",
    "session_runner.evaluate(validate_dataset, validate_labels)\n",
    "session_runner.evaluate(test_dataset, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(sample_image, label):\n",
    "    plt.figure()\n",
    "    plt.imshow(sample_image)\n",
    "    print('Label:', np.argmax(label))\n",
    "\n",
    "index=2\n",
    "show_sample(validate_dataset[index].reshape(features_shape[0:2]),validate_labels[index])\n",
    "np.argmax(session_runner.predict([validate_dataset[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
