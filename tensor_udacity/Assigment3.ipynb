{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_rel_path = '../datasets/notmnist/notMNIST.pickle'\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle location: C:\\Users\\lgess\\Documents\\repo\\learning_ai\\datasets\\notmnist\\notMNIST.pickle\n",
      "dict_keys(['train_labels', 'valid_labels', 'valid_dataset', 'test_dataset', 'test_labels', 'train_dataset'])\n"
     ]
    }
   ],
   "source": [
    "# Discover pickle path\n",
    "file_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "pickle_path = os.path.abspath(os.path.join(file_dir, pickle_rel_path))\n",
    "print('Pickle location:',pickle_path)\n",
    "\n",
    "# Load pickle\n",
    "with open(pickle_path,'rb') as file:\n",
    "    datasets = pickle.load(file)\n",
    "print(datasets.keys())\n",
    "\n",
    "# Set variables\n",
    "train_dataset = datasets['train_dataset']\n",
    "orig_train_labels = datasets['train_labels'] \n",
    "validate_dataset = datasets['valid_dataset']\n",
    "orig_validate_labels = datasets['valid_labels'] \n",
    "test_dataset = datasets['test_dataset']\n",
    "orig_test_labels = datasets['test_labels'] \n",
    "\n",
    "# Convert labels representation\n",
    "num_labels = 10\n",
    "def setup_labels(label_array, num_labels):\n",
    "    return ((np.arange(num_labels) == label_array[:,None]).astype(np.float32))\n",
    "train_labels = setup_labels(orig_train_labels, num_labels)\n",
    "validate_labels = setup_labels(orig_validate_labels, num_labels)\n",
    "test_labels = setup_labels(orig_test_labels, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    def __init__(self, w_shape, b_shape, name=\"\"):\n",
    "        self.name = name    \n",
    "        self.w_shape = w_shape\n",
    "        self.b_shape = b_shape\n",
    "        \n",
    "    def create_vars(self):\n",
    "        with tf.name_scope(self.name):\n",
    "            initializer = tf.glorot_normal_initializer()\n",
    "            self.w = tf.get_variable(name=\"W\"+self.name, shape=self.w_shape, dtype=tf.float32, initializer=initializer)\n",
    "            self.b = tf.get_variable(name=\"B\"+self.name, shape=self.b_shape, dtype=tf.float32, initializer=initializer)  \n",
    "    \n",
    "    def weighted_inputs(self, x): \n",
    "        with tf.name_scope(self.name):\n",
    "            w_rank = len(self.w_shape)\n",
    "            axis = np.arange(1 if w_rank > 1 else 0, w_rank)\n",
    "            return tf.tensordot(x,self.w,(axis,axis))+self.b\n",
    "    \n",
    "    def activation(self, x):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class HiddenLayer(Layer):       \n",
    "        \n",
    "    def activation(self, x):\n",
    "        with tf.name_scope(self.name):\n",
    "             return tf.nn.relu(self.weighted_inputs(x))\n",
    "                          \n",
    "class OutputLayer(Layer):\n",
    "        \n",
    "    def activation(self, x):\n",
    "        with tf.name_scope(self.name):\n",
    "            return tf.nn.softmax(self.weighted_inputs(x))\n",
    "                             \n",
    "    def loss(self, x, y):\n",
    "        with tf.name_scope(self.name):\n",
    "            logits = self.weighted_inputs(x)\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "            return tf.reduce_mean(cross_entropy)\n",
    "        \n",
    "class Model:\n",
    "                          \n",
    "    def __init__(self, layers, name=\"\"):\n",
    "        self.name = name\n",
    "        self.layers = layers   \n",
    "        \n",
    "    def create_vars(self):\n",
    "        for layer in self.layers:\n",
    "            layer.create_vars()\n",
    "                          \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.activation(x)\n",
    "        return x\n",
    "                          \n",
    "    def loss(self, x, y):  \n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer.activation(x)\n",
    "        return self.layers[-1].loss(x,y)\n",
    "    \n",
    "class ModelFactory:\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_simple_classifier(features_shape, num_neurons_array, num_labels):\n",
    "                            \n",
    "        layers = []    \n",
    "        w_shape = None\n",
    "        b_shape = features_shape\n",
    "        index = 1\n",
    "\n",
    "        for num_neurons in num_neurons_array:\n",
    "            w_shape = np.append([num_neurons],b_shape)\n",
    "            b_shape = [num_neurons]\n",
    "            layers.append(HiddenLayer(w_shape, b_shape, name=\"Layer\"+str(index)))\n",
    "            index+=1\n",
    "\n",
    "        w_shape = np.append([num_labels],b_shape)\n",
    "        b_shape = [num_labels]\n",
    "        layers.append(OutputLayer(w_shape, b_shape, name=\"\"+str(index))) \n",
    "\n",
    "        return Model(layers=layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionRunner:\n",
    "    \n",
    "    def __init__(self, features_shape, num_neurons_array, num_labels):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            self.session = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "            self.x = tf.placeholder(dtype=tf.float32,shape=np.append(None,features_shape), name=\"X\")\n",
    "            self.y = tf.placeholder(dtype=tf.float32,shape=np.append(None,[num_labels]), name=\"Y\")\n",
    "            model = ModelFactory.create_simple_classifier(\n",
    "                features_shape, num_neurons_array, num_labels)\n",
    "            model.create_vars()\n",
    "            self.predict = model.predict(self.x)\n",
    "            self.loss = model.loss(self.x,self.y)\n",
    "\n",
    "    def train(self, train_dataset, train_labels, batch_size=200, learning_rate=0.5, iterations=1000):\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            init = tf.global_variables_initializer()\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "            step = optimizer.minimize(self.loss)  \n",
    "        \n",
    "        self.session.run(init)\n",
    "        for i in range(iterations):        \n",
    "            index = (i*batch_size)%len((train_dataset)-batch_size)\n",
    "            feed_dict={self.x: train_dataset[index:index+batch_size], \n",
    "                       self.y: train_labels[index:index+batch_size]}      \n",
    "            self.session.run(step, feed_dict)\n",
    "            if i%(iterations/10) == 0:\n",
    "                loss_value = self.session.run(self.loss, feed_dict)\n",
    "                print(loss_value)\n",
    "        \n",
    "    def evaluate(self, num_predictions):\n",
    "        predicted = self.session.run(self.predict, feed_dict={self.x: validate_dataset[0:num_predictions]})\n",
    "        expected = validate_labels[0:num_predictions]\n",
    "        print(self.__accuracy(predicted, expected))\n",
    "        \n",
    "    def __accuracy(self, predicted, expected):\n",
    "        return np.mean(np.argmax(predicted,1) == np.argmax(expected,1))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7078801\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3f290d6527e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m                                \u001b[0mnum_neurons_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                num_labels = 10)\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession_runner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/tf.log\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession_runner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-9fed75fd01a9>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_dataset, train_labels, batch_size, learning_rate, iterations)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             feed_dict={self.x: train_dataset[index:index+batch_size], \n\u001b[0;32m     24\u001b[0m                        self.y: train_labels[index:index+batch_size]}      \n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "session_runner = SessionRunner(features_shape=train_dataset.shape[1:], \n",
    "                               num_neurons_array = [100],\n",
    "                               num_labels = 10)\n",
    "print(session_runner.train(train_dataset, train_labels, batch_size=200, learning_rate=0.5, iterations=1000))\n",
    "writer = tf.summary.FileWriter(\"/tf.log\", session_runner.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_runner.evaluate(num_predictions=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_runner.session.close()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(sample_image, label):\n",
    "    plt.figure()\n",
    "    plt.imshow(sample_image)\n",
    "    print('Label:', label)\n",
    "\n",
    "index=0\n",
    "show_sample(validate_dataset[index],validate_labels[index])\n",
    "sess.run(predict(),feed_dict={samples: [validate_dataset[index]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
