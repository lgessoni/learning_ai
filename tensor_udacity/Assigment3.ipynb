{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_rel_path = '../datasets/notmnist/notMNIST.pickle'\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle location: C:\\Users\\lgess\\Documents\\repo\\learning_ai\\datasets\\notmnist\\notMNIST.pickle\n",
      "dict_keys(['test_dataset', 'train_labels', 'test_labels', 'valid_dataset', 'train_dataset', 'valid_labels'])\n"
     ]
    }
   ],
   "source": [
    "# Discover pickle path\n",
    "file_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "pickle_path = os.path.abspath(os.path.join(file_dir, pickle_rel_path))\n",
    "print('Pickle location:',pickle_path)\n",
    "\n",
    "# Load pickle\n",
    "with open(pickle_path,'rb') as file:\n",
    "    datasets = pickle.load(file)\n",
    "print(datasets.keys())\n",
    "\n",
    "# Set variables\n",
    "train_dataset = datasets['train_dataset']\n",
    "orig_train_labels = datasets['train_labels'] \n",
    "validate_dataset = datasets['valid_dataset']\n",
    "orig_validate_labels = datasets['valid_labels'] \n",
    "test_dataset = datasets['test_dataset']\n",
    "orig_test_labels = datasets['test_labels'] \n",
    "\n",
    "# Convert labels representation\n",
    "num_labels = 10\n",
    "def setup_labels(label_array, num_labels):\n",
    "    return ((np.arange(num_labels) == label_array[:,None]).astype(np.float32))\n",
    "train_labels = setup_labels(orig_train_labels, num_labels)\n",
    "validate_labels = setup_labels(orig_validate_labels, num_labels)\n",
    "test_labels = setup_labels(orig_test_labels, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    def __init__(self, w_shape, b_shape, name=\"\"):\n",
    "        self.name = name    \n",
    "        self.w_shape = w_shape\n",
    "        self.b_shape = b_shape\n",
    "        \n",
    "    def create_vars(self):\n",
    "        with tf.name_scope(self.name):\n",
    "            initializer = tf.glorot_normal_initializer()\n",
    "            self.w = tf.get_variable(name=\"W\"+self.name, shape=self.w_shape, dtype=tf.float32, initializer=initializer)\n",
    "            self.b = tf.get_variable(name=\"B\"+self.name, shape=self.b_shape, dtype=tf.float32, initializer=initializer)  \n",
    "    \n",
    "    def weighted_inputs(self, x): \n",
    "        with tf.name_scope(self.name):\n",
    "            return tf.matmul(x, tf.transpose(self.w))+self.b\n",
    "    \n",
    "    def inference(self, x):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def training(self, x, y):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class HiddenLayer(Layer):       \n",
    "        \n",
    "    def inference(self, x):\n",
    "        with tf.name_scope(self.name):\n",
    "             return tf.nn.relu(self.weighted_inputs(x))\n",
    "            \n",
    "    def training(self, x, y, keep_prob):\n",
    "        with tf.name_scope(self.name):\n",
    "            x = tf.nn.dropout(x, keep_prob)\n",
    "            return self.inference(x)\n",
    "                          \n",
    "class OutputLayer(Layer):\n",
    "        \n",
    "    def inference(self, x):\n",
    "        with tf.name_scope(self.name):\n",
    "            logits = self.weighted_inputs(x)\n",
    "            return tf.nn.softmax(logits)\n",
    "                             \n",
    "    def training(self, x, y, keep_prob):\n",
    "        with tf.name_scope(self.name):\n",
    "            #x = tf.nn.dropout(x, keep_prob)\n",
    "            logits = self.weighted_inputs(x)\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "            loss = tf.reduce_mean(cross_entropy)\n",
    "            return loss\n",
    "        \n",
    "class Model:\n",
    "                          \n",
    "    def __init__(self, layers, name=\"\"):\n",
    "        self.name = name\n",
    "        self.layers = layers   \n",
    "        \n",
    "    def create_vars(self):\n",
    "        for layer in self.layers:\n",
    "            layer.create_vars()\n",
    "                          \n",
    "    def inference(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.inference(x)\n",
    "        return x\n",
    "                          \n",
    "    def loss(self, x, y, keep_prob, regularization):\n",
    "        l2 = 0\n",
    "        for layer in self.layers:\n",
    "            x = layer.training(x, y, keep_prob)\n",
    "            l2 = (l2 + tf.nn.l2_loss(layer.w)) if regularization > 0 else 0\n",
    "        return x + regularization*l2\n",
    "    \n",
    "def create_simple_classifier(features_shape, num_neurons_array, num_labels):\n",
    "                            \n",
    "    layers = []    \n",
    "    w_shape = None\n",
    "    b_shape = features_shape\n",
    "    index = 1\n",
    "\n",
    "    for num_neurons in num_neurons_array:\n",
    "        w_shape = np.append([num_neurons],b_shape)\n",
    "        b_shape = [num_neurons]\n",
    "        layers.append(HiddenLayer(w_shape, b_shape, name=\"Layer\"+str(index)))\n",
    "        index+=1\n",
    "\n",
    "    w_shape = np.append([num_labels],b_shape)\n",
    "    b_shape = [num_labels]\n",
    "    layers.append(OutputLayer(w_shape, b_shape, name=\"Layer\"+str(index))) \n",
    "\n",
    "    return Model(layers=layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionRunner:\n",
    "    \n",
    "    def __init__(self, features_shape, num_neurons_array, num_labels):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            self.session = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "            self.x = tf.placeholder(dtype=tf.float32,shape=np.append(None,features_shape), name=\"x\")\n",
    "            self.y = tf.placeholder(dtype=tf.float32,shape=np.append(None,[num_labels]), name=\"y\")\n",
    "            self.keep_prob = tf.placeholder(dtype=tf.float32,shape=[], name=\"keep_prob\")\n",
    "            self.regularization = tf.placeholder(dtype=tf.float32,shape=[], name=\"regularization\")\n",
    "            model = create_simple_classifier(features_shape, num_neurons_array, num_labels)\n",
    "            model.create_vars()\n",
    "            self.inference = model.inference(self.x)\n",
    "            self.loss = model.loss(self.x,self.y,self.keep_prob,self.regularization)\n",
    "\n",
    "    def train(self, train_dataset, train_labels, \n",
    "              batch_size=200, learning_rate=0.5, iterations=1000, keep_prob=0.75, regularization=0):\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            init = tf.global_variables_initializer()\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "            step = optimizer.minimize(self.loss, name=\"Optimizer\")  \n",
    "        \n",
    "        self.session.run(init)\n",
    "        for i in range(iterations):        \n",
    "            index = (i*batch_size)%(len(train_dataset)-batch_size)\n",
    "            feed_dict={self.x: train_dataset[index:index+batch_size], \n",
    "                       self.y: train_labels[index:index+batch_size],\n",
    "                       self.keep_prob: keep_prob,\n",
    "                       self.regularization: regularization}      \n",
    "            self.session.run(step, feed_dict)\n",
    "            if i%(iterations/10) == 0:\n",
    "                loss_value = self.session.run(self.loss, feed_dict)\n",
    "                print(loss_value)\n",
    "        \n",
    "    def evaluate(self, dataset, labels, num_predictions):\n",
    "        predicted = self.session.run(self.inference, feed_dict={self.x: dataset[0:num_predictions]})\n",
    "        expected = labels[0:num_predictions]\n",
    "        print(self.__accuracy(predicted, expected))\n",
    "        \n",
    "    def __accuracy(self, predicted, expected):\n",
    "        return np.mean(np.argmax(predicted,1) == np.argmax(expected,1))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2777174\n",
      "0.3167437\n",
      "0.25145963\n",
      "0.24783278\n",
      "0.22845748\n",
      "0.20748183\n",
      "0.17158107\n",
      "0.15065251\n",
      "0.12034005\n",
      "0.12492051\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    session_runner.session.close()\n",
    "except:\n",
    "    None\n",
    "session_runner = SessionRunner(features_shape=[28*28], \n",
    "                               num_neurons_array = [100,100,100,100],\n",
    "                               num_labels = (10))\n",
    "session_runner.train(train_dataset.reshape([-1,28*28]), train_labels, \n",
    "                     batch_size=1000, learning_rate=0.5, iterations=10000, keep_prob=0.99, regularization=0) \n",
    "writer = tf.summary.FileWriter(\"/tf.log\", session_runner.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95031\n",
      "0.8938\n",
      "0.952\n"
     ]
    }
   ],
   "source": [
    "session_runner.evaluate(train_dataset.reshape([-1,28*28]), train_labels, num_predictions=100000)\n",
    "session_runner.evaluate(validate_dataset.reshape([-1,28*28]), validate_labels, num_predictions=100000)\n",
    "session_runner.evaluate(test_dataset.reshape([-1,28*28]), test_labels, num_predictions=test_dataset.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_runner.session.close()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(sample_image, label):\n",
    "    plt.figure()\n",
    "    plt.imshow(sample_image)\n",
    "    print('Label:', label)\n",
    "\n",
    "index=0\n",
    "show_sample(validate_dataset[index],validate_labels[index])\n",
    "sess.run(predict(),feed_dict={samples: [validate_dataset[index]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
